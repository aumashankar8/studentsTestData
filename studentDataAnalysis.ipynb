{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Data Analysis Notebook\n",
    "This notebook houses the relevant data exploration and analysis for this test data set.\n",
    "Found here: https://www.kaggle.com/datasets/atharvbharaskar/students-test-data/\n",
    "\n",
    "There are 2 goals\n",
    "1. Determine 3 limitations of the data and determine any ways to enhance it.\n",
    "2. Analyze a particular trend and report the findings in written and verbal format with a slide deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Manipulation\n",
    "from pathlib import Path # Path reading\n",
    "import matplotlib.pyplot as plt # Plot creation\n",
    "import numpy as np # Data Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Default Plot\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an images folder and define a method for saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path() / \"images\" \n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path() / \"archive\" / \"Private_data.csv\"\n",
    "students = pd.read_csv(data_path)\n",
    "students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any Nulls\n",
    "null_count = students.isnull().sum()\n",
    "null_count #No null values to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some Value counts\n",
    "students.describe()\n",
    "print(students[\"PROGRAM NAME\"].value_counts())\n",
    "print(students[\"SEMESTER\"].value_counts())\n",
    "print(students[\"UNIVERSITY\"].value_counts())\n",
    "print(students[\"Domain\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations \n",
    "1. Documentation: Program Name, Rank, Percentile\n",
    "    The documentation indicates that there are supposed to be only 2 program names (BBA and MBA) instead, there are also the \"B.Com\" and \"Intg. BBA+MBA\" programs that are not described. While these can be taken into account through analysis, the purpose of the dataset was only for students pursuing BBA and MBA degrees. The rank and percentile columns are not defined in the documentation. Thus these columns are unable to be used because it is unclear where they are defined. Two students with the same score would have the same percentile if that was the only consideration, but that is not the case here so there must be more at play.\n",
    "2. Documentation: Domain and Test Description\n",
    "    The \"Domain\" column is said to \"indicate whether the exam was divided into two parts\". This seems to suggest a binary variable (either it IS or it ISN'T) but the column instead contains a categorical variable that relates to the specialization of that student as well as the \"Generic\" value. This would be fine but it causes some confusion when relating to the 2 different types of scores seen. If the domain is \"Generic\", then why is there a Domain Specific score at all? There is also the confusion with how the Test is defined. If a student's domain is \"generic\" then there should only be a General Management score (out of 100) and No domain score unless the \"generic\" domain is its own classification and then the \"Domain\" column itself is irrelevant because _every_ student would have both a General Management score and a Domain score. This second perspective is the assumption I will make in the further analysis.\n",
    "3. Data: \n",
    "    While the number of total students is adequate for analysis (N=151 isn't a large sample size but it is acceptable), there are a few columns with limited information on some of its labels. For example, Imperial College London, Caltech, and University of Cambridge each have only 2 or 1 students accounted for. This means that there will be difficulty predicting how students in these universities would score based on this data alone. \n",
    "## Suggestions for improvement\n",
    "More data would always be the first suggestion. As mentioned, a sample size of 151 is acceptable for a generic analysis but to get into the details of each of the columns, more information would help with extrapolation and prediction (specifically more students in each of the universities). \n",
    "Another general data suggestion would be to include information related to the students GPA (both overall as well as program specific) as a moderator. While GPA is not a perfect representation of a students' capability, it may be helpful as a moderator of score prediction. \n",
    "If any surveys after the assessment can be created additional information could be gathered such as graduation status and post-grad job status. These may provide additional insight on how this test may be an indicator of future success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal Bar Graph for University\n",
    "# print(plt.style.available)\n",
    "plt.style.use(\"seaborn-v0_8-dark\")\n",
    "fig, ax = plt.subplots()\n",
    "students[\"UNIVERSITY\"].value_counts().plot(kind = \"barh\", figsize=(10, 5), rot = 0)\n",
    "plt.xlabel(\"University\", labelpad=14)\n",
    "plt.ylabel(\"Number of Students\", labelpad=14)\n",
    "plt.title(\"Number of Students in each University\")\n",
    "plt.grid(axis = \"x\", linewidth = 0.5)\n",
    "save_fig(\"univeristy_graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal Bar Graph for Semester\n",
    "fig, ax = plt.subplots()\n",
    "students[\"SEMESTER\"].value_counts().plot(kind = \"barh\", figsize=(10, 5), rot = 0)\n",
    "plt.xlabel(\"Number of Students\", labelpad=14)\n",
    "plt.ylabel(\"Semester\", labelpad=14)\n",
    "plt.title(\"Number of Students in each Semester\")\n",
    "plt.grid(axis = \"x\", linewidth = 0.5)\n",
    "save_fig(\"semester_graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Graph to show the distribution of the Domain\n",
    "import seaborn as sns\n",
    "domain = students[\"Domain\"].value_counts()\n",
    "fig, axes = plt.subplots()\n",
    "plt.figure(figsize=(10,7))\n",
    "axes.pie(domain, labels = domain.index, startangle = 90, autopct=\"%1.1f%%\", radius=2, colors = sns.color_palette(\"deep\"))\n",
    "plt.show()\n",
    "save_fig(\"domain_pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Histograms for numerical data\n",
    "hist_students = students.drop([\"Unnamed: 0\"], axis = 1)\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax = fig.gca()\n",
    "hist_students.hist(ax = ax, bins = 20)\n",
    "fig.tight_layout()\n",
    "plt.grid(linewidth = 0.5)\n",
    "plt.show()\n",
    "save_fig(\"data_histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Graphs by Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# University Histogram\n",
    "university_percent = students.groupby([\"UNIVERSITY\"])[[\"PERCENTILE\"]].sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(figsize = (10,5))\n",
    "axes.barh(university_percent[\"UNIVERSITY\"], university_percent[\"PERCENTILE\"])\n",
    "# university_rank.plot(kind = \"barh\", x = \"UNIVERSITY\", y = \"PERCENTILE\", figsize=(10, 5), rot = 0, color = \"black\")\n",
    "plt.xlabel(\"Percentile\", labelpad=14)\n",
    "plt.ylabel(\"Number of students\", labelpad=14)\n",
    "plt.title(\"Rank Count by University\")\n",
    "plt.grid(axis = \"x\", linewidth = 0.5)\n",
    "save_fig(\"university_percentile_graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specialisation Grouped Graph\n",
    "spec_percent = students.groupby([\"Specialisation\"])[[\"PERCENTILE\"]].sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(figsize = (10,5))\n",
    "axes.barh(spec_percent[\"Specialisation\"], spec_percent[\"PERCENTILE\"])\n",
    "plt.xlabel(\"Percentile\", labelpad=14)\n",
    "plt.ylabel(\"Specialisation\", labelpad=14)\n",
    "plt.title(\"Percentile Count by Specialisation\")\n",
    "plt.grid(axis = \"x\", linewidth = 0.5)\n",
    "save_fig(\"specialisation_percentile_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Thoughts\n",
    "### Linear Regression\n",
    "What are some analysis that I can do with this data?\n",
    "\n",
    "Analysis Question: <br>\n",
    "How does program, semester, domain and university affect Percentile?\n",
    "- 4 Predictor Variables (Program, Semester, Domain, University) and 1 Outcome Variable (Percentile)\n",
    "- In future analysis, the outcome variable can be set to the Domain Specific Score and General Management Score\n",
    "- Let's start with a linear regression\n",
    "\n",
    "### Steps\n",
    "- Drop unneeded columns\n",
    "- Dummy Codes for the categorical varirables\n",
    "- Convert Semester to a Numerical Variable\n",
    "- Standardize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns\n",
    "# Convert Semester to Numerical\n",
    "# Grab and encode Categorical variables\n",
    "# Create a new data frame to keep the original data in tact\n",
    "students_data = students.drop([\"NAME OF THE STUDENT\",\"Unnamed: 0\"], axis = 1)\n",
    "students_data[\"SEMESTER\"].replace({\"5th\": 5, \"3rd\": 3, \"9th\": 9}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# students_data = students[[\"UNIVERSITY\",\"PROGRAM NAME\",\"Specialisation\", \"SEMESTER\",  \"Domain\", \"PERCENTILE\"]]\n",
    "encoder_cols = students_data.select_dtypes([\"object\"]).columns\n",
    "\n",
    "encoding = OrdinalEncoder()\n",
    "students_data[encoder_cols] = encoding.fit_transform(students_data[encoder_cols])\n",
    "X = students_data.drop([\"PERCENTILE\"], axis = 1)\n",
    "y = students_data[[\"PERCENTILE\"]].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = lin_reg.predict(X_test)\n",
    "print(y_prediction[:5])\n",
    "print(y_test[:5].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Scores\n",
    "lin_reg.score(X_train, y_train)\n",
    "lin_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Metrics\n",
    "- RMSE =~ 0\n",
    "- R<sup>2 </sup>= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the RMSE of this model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "lin_rmse = mean_squared_error(y_test, y_prediction)\n",
    "print(lin_rmse) #0.26 which is okay\n",
    "\n",
    "r2 = r2_score(y_test, y_prediction)\n",
    "print(r2) #0.22 Not Great\n",
    "\n",
    "# model_coef = lin_reg.named_steps['linearregression'].coef_\n",
    "# model_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-dark\")\n",
    "fig_scatter, ax_scatter = plt.subplots()\n",
    "\n",
    "ax_scatter.scatter(y_prediction, y_test)\n",
    "ax_scatter.set_xlabel(\"Linear Regression Prediction Values\")\n",
    "ax_scatter.set_ylabel(\"Test Values\")\n",
    "ax_scatter.set_title(\"Linear Regression: Test vs. Prediction Graph\")\n",
    "z_lin = np.polyfit(y_prediction.flatten(), y_test[\"PERCENTILE\"].values, 1)\n",
    "p_lin = np.poly1d(z_lin)\n",
    "\n",
    "plt.plot(y_prediction, p_lin(y_prediction))\n",
    "plt.grid(linewidth = 0.5)\n",
    "plt.show()\n",
    "save_fig(\"Linear_Regression Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Regressor\n",
    "Linear Regression seems to be overfitting <br>\n",
    "Let's Try SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter = 10000, tol = 1e-3)\n",
    "sgd_reg.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_prediction = sgd_reg.predict(X_test)\n",
    "print(sgd_prediction[:5])\n",
    "print(y[:5].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgd_score = sgd_reg.score(X_train, y_train)\n",
    "test_sgd_score = sgd_reg.score(X_test, y_test)\n",
    "print(train_sgd_score)\n",
    "print(test_sgd_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the RMSE of this model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "lin_rmse = mean_squared_error(y_test, sgd_prediction)\n",
    "print(lin_rmse) #0.004\n",
    "\n",
    "r2 = r2_score(y_test, sgd_prediction)\n",
    "print(r2) #0.936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-dark\")\n",
    "fig1_scatter, ax1_scatter = plt.subplots()\n",
    "ax1_scatter.scatter(sgd_prediction, y_test)\n",
    "ax1_scatter.set_xlabel(\"SGD Prediction Values\")\n",
    "ax1_scatter.set_ylabel(\"Test Values\")\n",
    "ax1_scatter.set_title(\"SGD: Test vs. Prediction\")\n",
    "\n",
    "z_sgd = np.polyfit(sgd_prediction.flatten(), y_test[\"PERCENTILE\"].values, 1)\n",
    "p_sgd = np.poly1d(z_sgd)\n",
    "\n",
    "plt.plot(sgd_prediction, p_sgd(sgd_prediction))\n",
    "plt.grid(linewidth = 0.5)\n",
    "\n",
    "plt.show() #reasonable and shows the skew of the scores a bit better\n",
    "\n",
    "save_fig(\"SGD_Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with R Code\n",
    "R can output a breakdown of the inputs so I can compare linear models easily\n",
    "Start with most Complex Model and move from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "students_data[\"Program_Name\"] = students_data[\"PROGRAM NAME\"] # API does not like spaces\n",
    "students_data_scaled = scaler.fit_transform(students_data)\n",
    "students_data_scaled = pd.DataFrame(students_data_scaled, columns=students_data.columns)\n",
    "model = ols(\"PERCENTILE ~ SEMESTER + Program_Name + Domain + UNIVERSITY + Specialisation\", data = students_data_scaled)\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "# R-squared = 0.236\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ols(\"PERCENTILE ~ SEMESTER\", data = students_data_scaled)\n",
    "results2 = model2.fit()\n",
    "# print(results2.summary()) #0.010\n",
    "\n",
    "model3 = ols(\"PERCENTILE ~ Program_Name\", data = students_data_scaled)\n",
    "results3 = model3.fit()\n",
    "# print(results3.summary()) #r^2 = 0.016\n",
    "\n",
    "model4 = ols(\"PERCENTILE ~ UNIVERSITY\", data = students_data_scaled)\n",
    "results4 = model.fit()\n",
    "# print(results4.summary()) #r^2 = 0.037\n",
    "\n",
    "model5 = ols(\"PERCENTILE ~ Specialisation\", data = students_data_scaled)\n",
    "results5 = model5.fit()\n",
    "print(results5.summary()) #r^2 = 0.001\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
